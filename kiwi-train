#!/usr/bin/python
# Copyright 2014 Douglas Bagnall <douglas@halo.gen.nz> LGPL/MPL2
import os, sys
import random
import itertools
import time
import argparse

DEFAULT_LEARN_RATE = 3e-5
DEFAULT_LEARN_RATE_DECAY = 1e-6
DEFAULT_LEARN_RATE_MIN = 1e-8
DEFAULT_DROPOUT = 0
DEFAULT_DROPOUT_DECAY = 0
DEFAULT_DROPOUT_MIN = 0.1
DEFAULT_MOMENTUM = 0.93
DEFAULT_MOMENTUM_SOFT_START = 5000
DEFAULT_LEARN_RATE_TIME_OFFSET = 2000.0

from classify import gst_init
from classify import Trainer, lr_sqrt_exp, lr_inverse_time
from classify import add_common_args, process_common_args

def main():
    gst_init()
    parser = argparse.ArgumentParser()
    add_common_args(parser)
    group = parser.add_argument_group('kiwi-train specific arguments')
    group.add_argument('-l', '--learn-rate', type=float, default=DEFAULT_LEARN_RATE,
                       help="learning rate")
    group.add_argument('--momentum', type=float, default=DEFAULT_MOMENTUM,
                       help="learning momentum")
    group.add_argument('--momentum-soft-start', type=float,
                       default=DEFAULT_MOMENTUM_SOFT_START,
                       help="high numbers slowly build up momentum")
    group.add_argument('--learn-rate-decay', type=float, default=DEFAULT_LEARN_RATE_DECAY,
                       help="learning rate decay")
    group.add_argument('--learn-rate-time-offset', type=float,
                       help="learning rate time offset (implies inverse-time schedule)")
    group.add_argument('--learn-rate-schedule', default='sqrt-exponential',
                       help='"sqrt-exponential", "inverse-time", or "flat"')
    group.add_argument('--learn-rate-min', type=float, default=DEFAULT_LEARN_RATE_MIN,
                       help="learning rate decay stops here")
    group.add_argument('--dropout', type=float, default=DEFAULT_DROPOUT,
                       help="dropout rate")
    group.add_argument('--dropout-decay', type=float, default=DEFAULT_DROPOUT_DECAY,
                       help="dropout decay")
    group.add_argument('--dropout-min', type=float, default=DEFAULT_DROPOUT_MIN,
                       help="dropout drops to zero from here")
    group.add_argument('-N', '--no-save-net', action='store_true',
                       help="don't save the net, periodically or otherwise")
    group.add_argument('-C', '--channels', default=12, type=int,
                       help="how many channels to use")
    group.add_argument('-r', '--random-alignment', action='store_true',
                       help="slightly randomise alignment of fft windows")
    group.add_argument('-E', '--error-weights',
                       help="multiply output errors by these ratios")
    group.add_argument('--bptt-depth', type=int,
                       help="Depth of BPTT training")
    group.add_argument('--activity-bias', type=int, default=1,
                       help="Train more on examples with changing classes")
    group.add_argument('-P', '--prioritise', action='store_true',
                       help="Do not renice downwards")
    args = parser.parse_args()

    if args.learn_rate_schedule == "flat":
        lr = lr_inverse_time(args.learn_rate, args.learn_rate)

    elif args.learn_rate_schedule == "inverse-time" or args.learn_rate_time_offset:
        lr = lr_inverse_time(args.learn_rate,
                             min(args.learn_rate_min, args.learn_rate),
                             offset=(args.learn_rate_time_offset or
                                     DEFAULT_LEARN_RATE_TIME_OFFSET))
    else:
        if args.learn_rate_schedule != "sqrt-exponential":
            print >> sys.stderr, "assuming sqrt-exponential schedule"
        lr = lr_sqrt_exp(args.learn_rate, args.learn_rate_decay,
                         min(args.learn_rate_min, args.learn_rate))

    dropout = lr_sqrt_exp(args.dropout, args.dropout_decay, args.dropout_min, 0)


    if not args.prioritise and hasattr(os, 'nice'):
        os.nice(10)

    n_channels = args.channels

    c = Trainer(channels=n_channels)
    c.no_save_net = args.no_save_net
    c.setp('random-alignment', args.random_alignment)
    if args.bptt_depth:
        c.setp('bptt-depth', args.bptt_depth)

    timed_files = process_common_args(c, args)

    validate_streams = [timed_files[:n_channels]]
    training_streams = [timed_files[n_channels:]]
    random.shuffle(training_streams[0])
    #--activity_bias=n: add n streams of tracks that actually have class changes,
    # but drop one file on each round to ensure they cycle out of sync
    for i in range(args.activity_bias):
        s  = [x for x in training_streams[0] if len(x.timings) > 1]
        if len(s) == 0: # < n_channels?
            break
        random.shuffle(s)
        training_streams.append(s[i:])

    if args.error_weights:
        c.setp('error-weight', args.error_weights)

    c.train(training_streams, validate_streams,
            iterations=args.iterations,
            learn_rate_fn=lr, dropout_fn=dropout,
            properties=(('momentum-soft-start', args.momentum_soft_start),
                        ('momentum', args.momentum),
                        #('momentum-style', 1),
                        ('weight-fan-in-sum', 4),
                        ('weight-fan-in-kurtosis', 0.15),
                    ))

main()
