#!/usr/bin/python
import os, sys
import random
import itertools
import time
import argparse

DEFAULT_LEARN_RATE = 3e-5
DEFAULT_LEARN_RATE_DECAY = 3e-5
DEFAULT_LEARN_RATE_MIN = 1e-8
DEFAULT_DROPOUT = 0
DEFAULT_DROPOUT_DECAY = 0
DEFAULT_DROPOUT_MIN = 0.1

from classify_kiwi import WINDOW_SIZE, BASENAME

from classify import Trainer, lr_sqrt_exp, gst_init
from classify import add_common_args, process_common_args

def main():
    gst_init()
    parser = argparse.ArgumentParser()
    add_common_args(parser, WINDOW_SIZE=WINDOW_SIZE,
                    BASENAME=BASENAME)
    group = parser.add_argument_group('kiwi-train specific arguments')
    group.add_argument('-l', '--learn-rate', type=float, default=DEFAULT_LEARN_RATE,
                       help="learning rate")
    group.add_argument('--learn-rate-decay', type=float, default=DEFAULT_LEARN_RATE_DECAY,
                       help="learning rate decay")
    group.add_argument('--learn-rate-min', type=float, default=DEFAULT_LEARN_RATE_MIN,
                       help="learning rate decay stops here")
    group.add_argument('--dropout', type=float, default=DEFAULT_DROPOUT,
                       help="dropout rate")
    group.add_argument('--dropout-decay', type=float, default=DEFAULT_DROPOUT_DECAY,
                       help="dropout decay")
    group.add_argument('--dropout-min', type=float, default=DEFAULT_DROPOUT_MIN,
                       help="dropout drops to zero from here")
    group.add_argument('-N', '--no-save-net', action='store_true',
                       help="don't save the net, periodically or otherwise")
    group.add_argument('-C', '--channels', default=12, type=int,
                       help="how many channels to use")
    group.add_argument('-r', '--random-alignment', action='store_true',
                       help="slightly randomise alignment of fft windows")
    group.add_argument('-E', '--error-weights',
                       help="multiply output errors by these ratios")
    group.add_argument('--bptt-depth', type=int,
                       help="Depth of BPTT training")
    group.add_argument('--activity-bias', type=int, default=1,
                       help="Train more on examples with changing classes")
    group.add_argument('-P', '--prioritise', type=int, default=False,
                       help="Do not renice downwards")

    args = parser.parse_args()

    lr = lr_sqrt_exp(args.learn_rate, args.learn_rate_decay, args.learn_rate_min)
    dropout = lr_sqrt_exp(args.dropout, args.dropout_decay, args.dropout_min, 0)


    if not args.prioritise and hasattr(os, 'nice'):
        os.nice(10)

    n_channels = args.channels

    c = Trainer(channels=n_channels)
    c.no_save_net = args.no_save_net
    c.setp('random-alignment', args.random_alignment)
    if args.bptt_depth:
        c.setp('bptt-depth', args.bptt_depth)

    timed_files = process_common_args(c, args)

    validate_streams = [timed_files[:n_channels]]
    training_streams = [timed_files[n_channels:]]
    random.shuffle(training_streams[0])
    #--activity_bias=n: add n streams of tracks that actually have class changes,
    # but drop one file on each round to ensure they cycle out of sync
    for i in range(args.activity_bias):
        s  = [x for x in training_streams[0] if len(x.timings) > 1]
        if len(s) == 0: # < n_channels?
            break
        random.shuffle(s)
        training_streams.append(s[i:])

    if args.error_weights:
        c.setp('error-weight', args.error_weights)

    c.train(training_streams, validate_streams,
            iterations=args.iterations,
            learn_rate_fn=lr, dropout_fn=dropout,
            properties=(('momentum-soft-start', 10000),
                        ('momentum', 0.93),
                        #('momentum-style', 1),
                        ('weight-fan-in-sum', 4),
                        ('weight-fan-in-kurtosis', 0.15),
                        #('weight-diagonal', 0.33),
                    ))

main()
