#!/usr/bin/python
# Copyright 2014 Douglas Bagnall <douglas@halo.gen.nz> LGPL
import os, sys
import random
import argparse

DEFAULT_LEARN_RATE = 3e-5
DEFAULT_LEARN_RATE_DECAY = 1e-6
DEFAULT_LEARN_RATE_MIN = 1e-8
DEFAULT_MOMENTUM = 0.93
DEFAULT_LEARNING_STYLE = 0
DEFAULT_MOMENTUM_SOFT_START = 5000
DEFAULT_LEARN_RATE_TIME_OFFSET = 2000.0

from classify import gst_init
from classify import Trainer, lr_sqrt_exp, lr_inverse_time
from classify import add_common_args, process_common_args

def main():
    gst_init()
    parser = argparse.ArgumentParser()
    add_common_args(parser)
    group = parser.add_argument_group('classify-train specific arguments')
    group.add_argument('-l', '--learn-rate', type=float, default=DEFAULT_LEARN_RATE,
                       help="learning rate")
    group.add_argument('--learning-style', type=int, default=DEFAULT_LEARNING_STYLE,
                       help="0: weighted, 1: Nesterov, 2: simplified N., "
                       "3: classical, 4: adagrad, 5:adadelta")
    group.add_argument('--momentum', type=float, default=DEFAULT_MOMENTUM,
                       help="learning momentum")
    group.add_argument('--momentum-soft-start', type=float,
                       default=DEFAULT_MOMENTUM_SOFT_START,
                       help="high numbers slowly build up momentum")
    group.add_argument('--learn-rate-decay', type=float, default=DEFAULT_LEARN_RATE_DECAY,
                       help="learning rate decay")
    group.add_argument('--learn-rate-time-offset', type=float,
                       help="learning rate time offset (implies inverse-time schedule)")
    group.add_argument('--learn-rate-schedule', default='sqrt-exponential',
                       help='"sqrt-exponential", "inverse-time", or "flat"')
    group.add_argument('--learn-rate-min', type=float, default=DEFAULT_LEARN_RATE_MIN,
                       help="learning rate decay stops here")
    group.add_argument('-N', '--no-save-net', action='store_true',
                       help="don't save the net, periodically or otherwise")
    group.add_argument('-C', '--channels', default=12, type=int,
                       help="how many channels to use")
    group.add_argument('-r', '--random-alignment', action='store_true',
                       help="slightly randomise alignment of fft windows")
    group.add_argument('-E', '--error-weights',
                       help="multiply output errors by these ratios")
    group.add_argument('--bptt-depth', type=int,
                       help="Depth of BPTT training")
    group.add_argument('--activity-bias', type=int, default=1,
                       help="Train more on examples with changing classes")
    group.add_argument('-P', '--prioritise', action='store_true',
                       help="Do not renice downwards")
    group.add_argument('--test-interval', type=int, default=0,
                       help="Test after this many training cycles")
    group.add_argument('--weight-noise', type=float, default=0.0,
                       help="std dev of noise added to weights before each cycle")
    group.add_argument('--presynaptic-noise', type=float, default=0.0,
                       help="std dev of noise added before non-linearity")
    group.add_argument('--weight-init-scale', type=float, default=0.0,
                       help="scale weights to approximately this gain")
    group.add_argument('--log-file', default="auto",
                       help="log to this file (default: based on net basename)")
    group.add_argument('--activation', default=1, type=int,
                       help="activation function (1: ReLU, 2: ReSQRT, 3: ReLOG, "
                       "4:ReTANH, 5: Clipped ReLU)")
    args = parser.parse_args()

    if args.learn_rate_schedule == "flat":
        lr = lr_inverse_time(args.learn_rate, args.learn_rate)

    elif args.learn_rate_schedule == "inverse-time" or args.learn_rate_time_offset:
        lr = lr_inverse_time(args.learn_rate,
                             min(args.learn_rate_min, args.learn_rate),
                             offset=(args.learn_rate_time_offset or
                                     DEFAULT_LEARN_RATE_TIME_OFFSET))
    else:
        if args.learn_rate_schedule != "sqrt-exponential":
            print >> sys.stderr, "assuming sqrt-exponential schedule"
        lr = lr_sqrt_exp(args.learn_rate, args.learn_rate_decay,
                         min(args.learn_rate_min, args.learn_rate))


    if not args.prioritise and hasattr(os, 'nice'):
        os.nice(10)

    n_channels = args.channels

    c = Trainer(channels=n_channels)
    c.no_save_net = args.no_save_net
    c.setp('random-alignment', args.random_alignment)
    if args.bptt_depth:
        c.setp('bptt-depth', args.bptt_depth)
    if args.test_interval > 0:
        c.test_interval = args.test_interval
    c.setp('weight-noise', args.weight_noise)
    c.setp('weight-init-scale', args.weight_init_scale)
    timed_files = process_common_args(c, args)

    validate_streams = [timed_files[:n_channels]]
    training_streams = [timed_files[n_channels:]]
    random.shuffle(training_streams[0])
    #--activity_bias=n: add n streams of tracks that actually have class changes,
    # but drop one file on each round to ensure they cycle out of sync
    for i in range(args.activity_bias):
        s  = [x for x in training_streams[0] if len(x.timings) > 1]
        if len(s) == 0: # < n_channels?
            break
        random.shuffle(s)
        training_streams.append(s[i:])

    if args.error_weights:
        c.setp('error-weight', args.error_weights)

    c.train(training_streams, validate_streams,
            iterations=args.iterations,
            learn_rate_fn=lr,
            log_file=args.log_file,
            properties=(('momentum-soft-start', args.momentum_soft_start),
                        ('momentum', args.momentum),
                        ('learning-style', args.learning_style),
                        #('weight-fan-in-sum', 4),
                        #('weight-fan-in-kurtosis', 0.15),
                        ('weight-init-method', 3),
                        ('presynaptic-noise', args.presynaptic_noise),
                        ('activation', args.activation)
                    ))

main()
